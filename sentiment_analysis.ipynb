{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run, scroll to the section at the bottom, edit the parameters and then run the entire notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages and defining pre-requesites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from typing import List, Dict, Set\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set\n",
    "from pandas.plotting import register_matplotlib_converters; register_matplotlib_converters()\n",
    "import urllib\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import pytz\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.ticker as mtick\n",
    "from progress.bar import IncrementalBar\n",
    "import time, sys\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading scraped data in to a DataFrame, and tagging brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the scraped YT data in to a nicely formatted DataFrame\n",
    "def return_data(directory):\n",
    "    df = pd.DataFrame(columns=['date', 'vid_id', 'author', 'comment'])\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename[-4:] == '.csv':\n",
    "                with open(os.path.join(directory, filename), 'r') as f:\n",
    "                    reader = csv.reader(f)\n",
    "                    df_temp = pd.DataFrame(list(reader))\n",
    "                df_temp = df_temp.fillna('')\n",
    "                # The below needs changing - the CSVs proved difficult to deal with so this is a bodged fix\n",
    "                df_temp['Comment'] = df_temp[list(range(7, max(df_temp.columns)+1))].agg(' '.join, axis=1)\n",
    "                z = pd.Series([s[:1] == '2' for s in df_temp[5]])\n",
    "                df_temp = df_temp[z]\n",
    "                df_temp_2 = pd.DataFrame()\n",
    "                df_temp_2['date'] = pd.to_datetime(df_temp[5][1:]).dt.tz_localize(None)\n",
    "                df_temp_2['comment'] = df_temp['Comment'][1:]\n",
    "                df_temp_2['vid_id'] = df_temp[0][1:]\n",
    "                df_temp_2['author'] = df_temp[4][1:]\n",
    "                df = df.append(df_temp_2, sort=True)\n",
    "    df = df.drop_duplicates().sort_values(by='date').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Function to collect data from all channels in to one DataFrame\n",
    "# folder_loc should be a folder containing a folder for each channel, inside of which should be the scraped comments\n",
    "def tagged_comm_all_channels(folder_loc, start_date, end_date, brands=[]):\n",
    "    print('Collecting comments in to a dataframe and tagging brands', '\\n')\n",
    "    output_df = pd.DataFrame(columns=['date', 'vid_id', 'author', 'comment', 'channel'])\n",
    "\n",
    "    t_0 = time.time()\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_loc):\n",
    "        for name in dirs:\n",
    "            if name != 'YouTube comments':\n",
    "                print(f'Starting: {os.path.join(root, name)}')\n",
    "                channel_df = return_data(os.path.join(root, name))\n",
    "                channel_df['channel'] = [name.replace('_', ' ') for _ in range(len(channel_df))]\n",
    "                output_df = output_df.append(channel_df, sort=False)\n",
    "                print('Folder complete \\n')\n",
    "\n",
    "    output_df['comment_length'] = [len(c) for c in output_df['comment']]\n",
    "    output_df = output_df[['date', 'channel', 'vid_id', 'author', 'comment', 'comment_length']]\n",
    "    output_df = output_df[(output_df['date'] >= pd.datetime(*start_date)) & (output_df['date'] <= pd.datetime(*end_date))]\n",
    "    output_df['tags'] = [', '.join([brand for brand in brands if brand.lower() in comment.lower()]) for comment in output_df['comment']]\n",
    "    \n",
    "    output_df.to_csv(f'comments_{start_date}_to_{end_date}.csv', index=False)\n",
    "    \n",
    "    print(f'Done after {round((time.time() - t_0), 2)} seconds.')\n",
    "    \n",
    "    return output_df.sort_values(by='date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with the Google NLP API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify the sentiment of a string. Outputs the score and magnitude, unless print_results is True.\n",
    "def classify(string, print_results=False):\n",
    "    document = types.Document(content=string, type=enums.Document.Type.PLAIN_TEXT)\n",
    "    \n",
    "    # Instantiates a client\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    # Detects the sentiment of the text\n",
    "    sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    \n",
    "    # Outputs the results\n",
    "    if not print_results:\n",
    "        return (sentiment.score, sentiment.magnitude)\n",
    "    else:\n",
    "        print(f'Text: {string}')\n",
    "        print(f'Sentiment: {sentiment.score}, {sentiment.magnitude} \\n')\n",
    "        \n",
    "        \n",
    "# Seconds to hh mm ss function\n",
    "def display_time_passed(secs):\n",
    "    print(f\"{secs//60**2} hours {(secs%60**2)//60} minutes {secs%60} seconds\")\n",
    "    \n",
    "    \n",
    "# Function converting a tuple to a string\n",
    "def to_date_string(date_tuple):\n",
    "    date_list = [str(a).zfill(2) for a in list(date_tuple)]\n",
    "    return '-'.join(date_list)\n",
    "\n",
    "\n",
    "# Progress bar function\n",
    "def update_progress(time_start, comments_done, comments_total, string=\"\"):\n",
    "    progress = comments_done / comments_total\n",
    "    bar_length = 30\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(string)\n",
    "    print(text)\n",
    "    print(f\"Comments analysed: {comments_done} of {comments_total}\")\n",
    "    print(f\"Time elapsed: {round(time.time() - time_start,1)} seconds\")\n",
    "    \n",
    "    \n",
    "# Function to update a dataframe with sentiment score and magnitude.\n",
    "def sentiment_analysis_df(df, save_loc, debug=False):\n",
    "    print('\\n\\n',  'Analysing sentiment of tagged channels', '\\n')\n",
    "    time_start = time.time()\n",
    "    initial_string = f\"Start time: {datetime.datetime.fromtimestamp(time_start).strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    print(initial_string)\n",
    "    error_dict = {}\n",
    "    \n",
    "    df_new = df[df['tags']!='']\n",
    "    df_new['sentiment_score'] = np.zeros(len(df_new))\n",
    "    df_new['sentiment_magnitude'] = np.zeros(len(df_new))\n",
    "\n",
    "    tagged_comments_length = len(df_new)\n",
    "    \n",
    "    for i in range(tagged_comments_length):\n",
    "        try:\n",
    "            score, magnitude = classify(df_new['comment'].iloc[i])\n",
    "            df_new['sentiment_score'].iloc[i] = score\n",
    "            df_new['sentiment_magnitude'].iloc[i] = magnitude\n",
    "        except Exception as err:\n",
    "            error_dict[i] = err\n",
    "\n",
    "        if not debug:\n",
    "            update_progress(time_start, i, tagged_comments_length, initial_string)\n",
    "        else:\n",
    "            print(f'Finished {i} of {tagged_comments_length}')\n",
    "        \n",
    "        if i%30 == 0:\n",
    "            df_new.to_csv(os.path.join(save_loc, \"temp_save_file.csv\"))\n",
    "    \n",
    "    if not debug:\n",
    "        update_progress(time_start, tagged_comments_length, tagged_comments_length, initial_string)\n",
    "        \n",
    "    df_new.to_csv(os.path.join(save_loc, \"temp_save_file.csv\"))\n",
    "    df_new.to_csv(os.path.join(save_loc, f\"comments_sentiment_{to_date_string(start_date)}_to_{to_date_string(end_date)}.csv\"), index=False)\n",
    "    \n",
    "    print(\"\\nCompleted\", (error_dict!={})*\"with errors:\")\n",
    "    for key, val in error_dict.items():\n",
    "        print(f\"- Error on comment {key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "Edit the below parameters before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials are set here. This must be changed before running.\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"CREDENTIALS.json\"\n",
    "\n",
    "# Set the dates between which comments will be analysed\n",
    "start_date, end_date = (2020, 2, 1), (2020, 4, 30)\n",
    "\n",
    "# Folder location in which comment .csv files are saved, separated in to folders for each channel\n",
    "parent_folder_loc = 'COMMENTS LOCATION HERE'\n",
    "\n",
    "# Brands to look at specifically\n",
    "brands_feb_to_apr = ['Amp Human', 'Bell', 'Bentonville', 'Douchebags', 'On Running', 'Polar', 'Pole', 'Quarq', 'Shimano', 'Zipp', 'SRAM']\n",
    "\n",
    "# Temporary location to save results, useful if script crashes mid-way through analysis\n",
    "comments_save_location = 'SAVE LOCATION HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tagged comments df and csv\n",
    "comments_df_new = tagged_comm_all_channels(parent_folder_loc, start_date, end_date, brands_feb_to_apr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse comment sentiment for brands\n",
    "sentiment_analysis_df(comments_df_new, comments_save_location, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
